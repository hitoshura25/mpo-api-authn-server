name: Create Fine-Tuning Dataset

on:
  workflow_dispatch:
    inputs:
      analysis_run_id:
        description: 'Run ID of the OLMo analysis to use (leave empty for latest)'
        required: false
        type: string
      upload_to_hf:
        description: 'Upload to Hugging Face Hub'
        required: false
        type: boolean
        default: false

  workflow_run:
    workflows: ["OLMo Security Analysis"]
    types:
      - completed

jobs:
  prepare-dataset:
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          cd security-ai-analysis
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install huggingface_hub
      
      - name: Download latest analysis results
        run: |
          cd security-ai-analysis
          
          # Get the latest analysis artifact
          if [ -n "${{ inputs.analysis_run_id }}" ]; then
            RUN_ID="${{ inputs.analysis_run_id }}"
          else
            # Get latest successful analysis run
            RUN_ID=$(gh run list \
              --workflow "OLMo Security Analysis" \
              --status success \
              --limit 1 \
              --json databaseId \
              --jq '.[0].databaseId')
          fi
          
          echo "Using analysis from run: $RUN_ID"
          
          # Download the analysis results
          gh run download $RUN_ID \
            --name "olmo-security-analysis-*" \
            --dir data/ || {
              echo "Could not find analysis artifacts. Running fresh analysis..."
              python process_artifacts.py
            }
        env:
          GH_TOKEN: ${{ github.token }}
      
      - name: Create narrativized dataset
        run: |
          cd security-ai-analysis
          echo "üìù Creating narrativized security dataset..."
          python create_narrativized_dataset.py
          echo "‚úÖ Narrativization complete"
      
      - name: Prepare fine-tuning dataset
        id: prepare
        run: |
          cd security-ai-analysis
          echo "üîß Preparing fine-tuning dataset..."
          python prepare_finetuning_dataset.py
          
          # Get dataset stats
          if [ -d "data/finetuning_dataset" ]; then
            DATASET_SIZE=$(du -sh data/finetuning_dataset | cut -f1)
            TRAIN_COUNT=$(wc -l < data/finetuning_dataset/train.jsonl)
            VAL_COUNT=$(wc -l < data/finetuning_dataset/validation.jsonl)
            
            echo "dataset_size=$DATASET_SIZE" >> $GITHUB_OUTPUT
            echo "train_count=$TRAIN_COUNT" >> $GITHUB_OUTPUT
            echo "val_count=$VAL_COUNT" >> $GITHUB_OUTPUT
            
            # Create zip for easy download
            zip -r finetuning_dataset.zip data/finetuning_dataset/
            echo "üì¶ Created finetuning_dataset.zip"
          fi
      
      - name: Generate Colab notebook
        run: |
          cd security-ai-analysis
          
          cat > olmo_security_finetune.ipynb << 'EOF'
          {
            "cells": [
              {
                "cell_type": "markdown",
                "metadata": {"id": "header"},
                "source": [
                  "# üîí Fine-Tune OLMo for Security Remediation\n",
                  "\n",
                  "This notebook fine-tunes OLMo-1B on your WebAuthn security dataset.\n",
                  "\n",
                  "**Prerequisites:**\n",
                  "1. Upload `finetuning_dataset.zip` to your Google Drive\n",
                  "2. Enable GPU runtime: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n"
                ]
              },
              {
                "cell_type": "code",
                "execution_count": null,
                "metadata": {"id": "install"},
                "outputs": [],
                "source": [
                  "# Install required packages\n",
                  "!pip install -q transformers datasets torch accelerate huggingface_hub"
                ]
              },
              {
                "cell_type": "code",
                "execution_count": null,
                "metadata": {"id": "mount"},
                "outputs": [],
                "source": [
                  "# Mount Google Drive\n",
                  "from google.colab import drive\n",
                  "drive.mount('/content/drive')\n",
                  "\n",
                  "# Copy and extract dataset\n",
                  "!cp /content/drive/MyDrive/finetuning_dataset.zip .\n",
                  "!unzip -q finetuning_dataset.zip\n",
                  "print('‚úÖ Dataset extracted')\n",
                  "!ls -la data/finetuning_dataset/"
                ]
              },
              {
                "cell_type": "code",
                "execution_count": null,
                "metadata": {"id": "load"},
                "outputs": [],
                "source": [
                  "# Load model and dataset\n",
                  "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                  "from datasets import load_from_disk\n",
                  "import torch\n",
                  "\n",
                  "print('Loading OLMo-1B...')\n",
                  "model = AutoModelForCausalLM.from_pretrained(\n",
                  "    'allenai/OLMo-1B',\n",
                  "    trust_remote_code=True,\n",
                  "    torch_dtype=torch.float16,\n",
                  "    device_map='auto'\n",
                  ")\n",
                  "tokenizer = AutoTokenizer.from_pretrained(\n",
                  "    'allenai/OLMo-1B',\n",
                  "    trust_remote_code=True\n",
                  ")\n",
                  "\n",
                  "# Set pad token\n",
                  "if tokenizer.pad_token is None:\n",
                  "    tokenizer.pad_token = tokenizer.eos_token\n",
                  "\n",
                  "print('Loading dataset...')\n",
                  "dataset = load_from_disk('data/finetuning_dataset/olmo_security_dataset')\n",
                  "print(f'Dataset: {len(dataset[\"train\"])} train, {len(dataset[\"validation\"])} validation')"
                ]
              },
              {
                "cell_type": "code",
                "execution_count": null,
                "metadata": {"id": "train"},
                "outputs": [],
                "source": [
                  "# Fine-tune the model\n",
                  "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
                  "\n",
                  "# Tokenize\n",
                  "def tokenize_function(examples):\n",
                  "    return tokenizer(\n",
                  "        examples['text'],\n",
                  "        truncation=True,\n",
                  "        padding='max_length',\n",
                  "        max_length=512\n",
                  "    )\n",
                  "\n",
                  "print('Tokenizing dataset...')\n",
                  "tokenized = dataset.map(tokenize_function, batched=True)\n",
                  "\n",
                  "# Training arguments optimized for Colab\n",
                  "training_args = TrainingArguments(\n",
                  "    output_dir='./olmo-security-finetuned',\n",
                  "    num_train_epochs=3,\n",
                  "    per_device_train_batch_size=2,  # Small batch for T4\n",
                  "    gradient_accumulation_steps=4,   # Effective batch size of 8\n",
                  "    per_device_eval_batch_size=4,\n",
                  "    warmup_steps=100,\n",
                  "    weight_decay=0.01,\n",
                  "    logging_steps=25,\n",
                  "    save_steps=250,\n",
                  "    eval_steps=250,\n",
                  "    evaluation_strategy='steps',\n",
                  "    save_total_limit=2,\n",
                  "    load_best_model_at_end=True,\n",
                  "    metric_for_best_model='eval_loss',\n",
                  "    fp16=True,\n",
                  "    report_to='none'\n",
                  ")\n",
                  "\n",
                  "# Trainer\n",
                  "trainer = Trainer(\n",
                  "    model=model,\n",
                  "    args=training_args,\n",
                  "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
                  "    train_dataset=tokenized['train'],\n",
                  "    eval_dataset=tokenized['validation']\n",
                  ")\n",
                  "\n",
                  "print('üöÄ Starting training...')\n",
                  "trainer.train()\n",
                  "print('‚úÖ Training complete!')"
                ]
              },
              {
                "cell_type": "code",
                "execution_count": null,
                "metadata": {"id": "save"},
                "outputs": [],
                "source": [
                  "# Save the model\n",
                  "print('Saving model...')\n",
                  "trainer.save_model('./olmo-security-finetuned')\n",
                  "tokenizer.save_pretrained('./olmo-security-finetuned')\n",
                  "\n",
                  "# Save to Drive\n",
                  "!cp -r ./olmo-security-finetuned /content/drive/MyDrive/\n",
                  "print('‚úÖ Model saved to Google Drive')"
                ]
              },
              {
                "cell_type": "code",
                "execution_count": null,
                "metadata": {"id": "test"},
                "outputs": [],
                "source": [
                  "# Test the fine-tuned model\n",
                  "test_prompts = [\n",
                  "    \"Fix the GitHub Actions permission vulnerability:\",\n",
                  "    \"How to secure WebAuthn credential validation:\",\n",
                  "    \"Generate a patch for SQL injection:\"\n",
                  "]\n",
                  "\n",
                  "for prompt in test_prompts:\n",
                  "    print(f\"\\nüîç Prompt: {prompt}\")\n",
                  "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
                  "    outputs = model.generate(\n",
                  "        **inputs,\n",
                  "        max_new_tokens=150,\n",
                  "        temperature=0.3,\n",
                  "        do_sample=True,\n",
                  "        pad_token_id=tokenizer.pad_token_id\n",
                  "    )\n",
                  "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                  "    print(f\"Response: {response}\")\n",
                  "    print(\"-\" * 50)"
                ]
              },
              {
                "cell_type": "code",
                "execution_count": null,
                "metadata": {"id": "push_hf"},
                "outputs": [],
                "source": [
                  "# Optional: Push to Hugging Face Hub\n",
                  "# from huggingface_hub import notebook_login\n",
                  "# notebook_login()\n",
                  "# \n",
                  "# trainer.push_to_hub('your-username/olmo-security-finetuned', private=True)\n",
                  "# tokenizer.push_to_hub('your-username/olmo-security-finetuned', private=True)"
                ]
              }
            ],
            "nbformat": 4,
            "nbformat_minor": 0
          }
          EOF
          
          echo "‚úÖ Created Colab notebook"
      
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: olmo-finetuning-dataset-${{ github.run_id }}
          path: |
            security-ai-analysis/finetuning_dataset.zip
            security-ai-analysis/olmo_security_finetune.ipynb
            security-ai-analysis/data/finetuning_dataset/train.jsonl
            security-ai-analysis/data/finetuning_dataset/validation.jsonl
          retention-days: 90
      
      - name: Upload to Hugging Face Hub
        if: inputs.upload_to_hf && env.HF_TOKEN != ''
        run: |
          cd security-ai-analysis
          
          python << 'EOF'
          from huggingface_hub import HfApi, create_repo
          from datasets import load_from_disk
          import os
          
          token = os.environ.get('HF_TOKEN')
          username = os.environ.get('HF_USERNAME', 'your-username')
          
          # Load and upload dataset
          dataset = load_from_disk("data/finetuning_dataset/olmo_security_dataset")
          repo_id = f"{username}/webauthn-security-finetune"
          
          try:
              create_repo(repo_id, repo_type="dataset", token=token, private=True, exist_ok=True)
              dataset.push_to_hub(repo_id, token=token, private=True)
              print(f"‚úÖ Dataset uploaded to: https://huggingface.co/datasets/{repo_id}")
          except Exception as e:
              print(f"‚ùå Upload failed: {e}")
          EOF
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_USERNAME: ${{ secrets.HF_USERNAME }}
      
      - name: Generate summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY << EOF
          # üéØ Fine-Tuning Dataset Created
          
          ## üìä Dataset Statistics
          - **Size**: ${{ steps.prepare.outputs.dataset_size }}
          - **Training Examples**: ${{ steps.prepare.outputs.train_count }}
          - **Validation Examples**: ${{ steps.prepare.outputs.val_count }}
          
          ## üì• Download & Use
          
          ### Quick Start with Google Colab:
          1. Download artifact: \`olmo-finetuning-dataset-${{ github.run_id }}\`
          2. Upload \`finetuning_dataset.zip\` to your Google Drive
          3. Open \`olmo_security_finetune.ipynb\` in Colab
          4. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU
          5. Run all cells
          
          ### What You'll Get:
          - Fine-tuned OLMo that generates actual security fixes
          - Model saved to your Google Drive
          - Ready to use for remediation generation
          
          ### Files Included:
          - \`finetuning_dataset.zip\` - Complete dataset
          - \`olmo_security_finetune.ipynb\` - Ready-to-run Colab notebook
          - \`train.jsonl\` - Training examples
          - \`validation.jsonl\` - Validation examples
          
          ## üöÄ Expected Improvements After Fine-Tuning:
          - ‚úÖ Specific code patches instead of generic advice
          - ‚úÖ WebAuthn-specific security fixes
          - ‚úÖ Actionable remediation steps
          - ‚úÖ Better understanding of your codebase patterns
          EOF
