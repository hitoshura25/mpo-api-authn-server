# AI Security Fine-Tuning Usage Guide

## Overview

This guide documents all integration modes and usage patterns for the AI Security Fine-Tuning system (Phase 3 of the AI Security Dataset Research Initiative). The system provides flexible execution modes for different development and production scenarios.

üìñ **Quick Links:**
- [‚Üê Back to Main README](../../security-ai-analysis/README.md) - System overview and basic setup
- [üõ†Ô∏è MLX Installation Guide](mlx-installation-guide.md) - Prerequisites for fine-tuning setup

---

**Target Audience**: Developers and researchers using the AI security analysis pipeline  
**Prerequisites**: MLX installed and configured ‚Üí **[MLX Installation Guide](mlx-installation-guide.md)**

## Architecture Summary

### 4-Phase AI Enhancement Pipeline
1. **Phase 1: Enhanced Dataset Creation** ‚Üí Professional FOSS tools scan code ‚Üí 5x enhanced security patterns ‚Üí Rich training datasets
2. **Phase 2: RAG-Enhanced Analysis** ‚Üí Context-aware vulnerability analysis with retrieval augmentation ‚Üí Detailed narratives
3. **Phase 3: Sequential Fine-Tuning** ‚≠ê **NEW DEFAULT** ‚Üí Progressive specialization with two models:
   - **Stage 1**: Vulnerability Analysis Specialist (base model ‚Üí analysis expert)
   - **Stage 2**: Code Fix Generation Specialist (Stage 1 model ‚Üí code fix expert)
4. **Phase 4: Production Upload** ‚Üí Specialized models and datasets published to HuggingFace Hub

### Default Behavior
- **Sequential fine-tuning enabled by default** in all modes (creates two specialized models for maximum accuracy)
- **Automatic fallback** to single-stage fine-tuning if sequential modules unavailable
- **Opt-out available** for development/testing scenarios
- **Fail-fast approach** for system errors, graceful handling for legitimate opt-outs

## Integration Modes

### 1. Manual Mode (Development & Testing)

#### 1.1 Standard Execution (Default)
Complete 4-phase AI enhancement pipeline with sequential fine-tuning:

```bash
# Navigate to security analysis directory
cd security-ai-analysis

# Run complete pipeline with sequential fine-tuning (default behavior)
python3 process_artifacts.py

# Complete pipeline with model upload to HuggingFace
python3 process_artifacts.py --upload-model

# Disable sequential fine-tuning, use single-stage approach
python3 process_artifacts.py --disable-sequential-fine-tuning

# With custom model and upload
python3 process_artifacts.py --model-name ~/shared-olmo-models/base/OLMo-2-1B-Instruct --upload-model

# With branch/commit tracking and upload
python3 process_artifacts.py --branch main --commit abc123def --upload-model
```

**Expected Output**:
```
üîí WebAuthn Security Analysis with OLMo-2-1B
=============================================================
Phase 1: Enhanced Dataset Creation ‚úÖ
Phase 2: RAG-Enhanced Analysis ‚úÖ
Phase 3: Sequential Fine-Tuning (New Default Behavior) ‚úÖ
  üéØ Using Phase 3: Sequential Fine-Tuning (Default)
  üìä Processing 233 vulnerabilities for sequential training
  üìÅ Stage 1: Training Vulnerability Analysis Specialist...
  ‚úÖ Stage 1 completed in 19.7s
  üìÅ Stage 2: Training Code Fix Generation Specialist...
  ‚úÖ Stage 2 completed in 20.8s
  üéâ Sequential fine-tuning completed successfully!
  üìÅ Stage 1 model: ~/shared-olmo-models/fine-tuned/webauthn-security-sequential_20250917_123050_stage1_analysis
  üìÅ Stage 2 model: ~/shared-olmo-models/fine-tuned/webauthn-security-sequential_20250917_123050_stage2_codefix
Phase 4: Production Upload ‚úÖ
  ‚úÖ SUCCESS: Production dataset updated!
  üîó Dataset URL: https://huggingface.co/datasets/hitoshura25/webauthn-security-vulnerabilities-olmo
```

#### 1.2 Development Mode (Opt-Out)
Options for faster iteration during development:

```bash
# Disable sequential fine-tuning, use faster single-stage approach
python3 process_artifacts.py --disable-sequential-fine-tuning

# Skip fine-tuning entirely (fastest for development/testing)
python3 process_artifacts.py --disable-sequential-fine-tuning --skip-fine-tuning

# Combine with other options  
python3 process_artifacts.py --disable-sequential-fine-tuning --branch develop

# Note: --upload-model has no effect when --skip-fine-tuning is used
```

**When to Use Opt-Out**:
- üß™ **Development/Testing**: Faster iteration during pipeline development
- üö® **Emergency Mode**: If fine-tuning breaks, temporary disable
- üìä **Dataset-Only Runs**: Rare cases where only training data needed

#### 1.3 Manual Artifact Processing
Process existing artifacts with fine-tuning:

```bash
# Process artifacts from specific directory
python3 process_artifacts.py --artifacts-dir data/custom_security_artifacts

# Process with custom output directory
python3 process_artifacts.py --output-dir results/security_analysis_$(date +%Y%m%d)
```

### 2. Automated Daemon Mode (Production)

#### 2.1 Daemon Configuration
Fine-tuning runs automatically every 5 minutes via LaunchAgent:

```bash
# Check daemon status
python3 setup_launchagent.py --status

# View daemon configuration
cat templates/daemon.plist.template
```

**Daemon Configuration** (`local-analysis/security_artifact_daemon.py`):
- **Polling**: Every 5 minutes via LaunchAgent
- **Auto-Download**: Latest artifacts from main branch GitHub Actions
- **Complete Pipeline**: All 5 phases including fine-tuning
- **Error Handling**: Continues operation on fine-tuning failures

#### 2.2 Emergency Override
Disable fine-tuning in daemon mode (emergency only):

```yaml
# config/olmo-security-config.yaml
fine_tuning:
  skip_in_daemon: true  # Emergency override - disable automated fine-tuning
  # Set back to false when issue resolved
```

**When to Use Emergency Override**:
- üö® **Fine-tuning broken**: Temporary disable while fixing issues
- üîß **System maintenance**: Prevent fine-tuning during maintenance
- üìä **Resource constraints**: Temporary resource limitation

**Note**: This is an emergency measure - normal daemon operation includes fine-tuning.

### 3. Standalone Mode (Advanced)

#### 3.1 Direct MLX Fine-Tuning
Advanced users can run fine-tuning directly on existing datasets:

```bash
# Navigate to security analysis directory
cd security-ai-analysis

# Fine-tune with existing dataset
python3 scripts/mlx_finetuning.py --dataset data/train_20250912_143022.jsonl

# With custom output name and upload
python3 scripts/mlx_finetuning.py \
  --dataset data/custom_security_data.jsonl \
  --output-name webauthn-custom-model-$(date +%Y%m%d) \
  --upload

# With custom configuration
python3 scripts/mlx_finetuning.py \
  --dataset data/security_data.jsonl \
  --batch-size 4 \
  --learning-rate 1e-5 \
  --max-epochs 3
```

#### 3.2 Standalone Options
```bash
# Show all available options
python3 scripts/mlx_finetuning.py --help
```

**Key Parameters**:
- `--dataset`: Path to JSONL training dataset (required)
- `--output-name`: Custom fine-tuned model name
- `--upload`: Upload to HuggingFace after fine-tuning
- `--batch-size`: Training batch size (default: 2)
- `--learning-rate`: Learning rate (default: 1e-5)
- `--max-epochs`: Maximum training epochs (default: 3)
- `--base-model`: Base model to fine-tune (default: from config)

## Configuration Management

### Configuration Files

#### Primary Configuration
**File**: `config/olmo-security-config.yaml`

```yaml
fine_tuning:
  # Core settings
  skip_in_daemon: false          # Emergency override only
  base_model_name: "OLMo-2-1B-Instruct"
  workspace_dir: "fine-tuning"
  
  # Training parameters
  batch_size: 2
  learning_rate: 1e-5
  max_epochs: 3
  warmup_steps: 100
  save_steps: 500
  eval_steps: 250
  
  # MLX optimization
  gradient_checkpointing: true
  quantization: "none"          # or "4bit", "8bit"
  
  # Output settings
  output_model_prefix: "webauthn-security-olmo"
  
  # HuggingFace upload
  upload_enabled: true          # Upload fine-tuned models
  hf_repo_prefix: "hitoshura25/webauthn-security-model"
```

#### Environment Variable Overrides
```bash
# Override configuration via environment variables
export FINE_TUNING_BATCH_SIZE=4
export FINE_TUNING_LEARNING_RATE=2e-5
export FINE_TUNING_UPLOAD_ENABLED=true

# HuggingFace token for model upload (required for --upload-model)
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# Run with overrides
python3 process_artifacts.py
```

#### HuggingFace Model Upload Setup
```bash
# 1. Get HuggingFace token from https://huggingface.co/settings/tokens
# 2. Set environment variable
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# 3. Test token validity
python3 -c "from huggingface_hub import whoami; print(whoami())"

# 4. Run with model upload
python3 process_artifacts.py --upload-model
```

### Workspace Structure
```
fine-tuning/
‚îú‚îÄ‚îÄ training_data/           # Generated training datasets
‚îÇ   ‚îú‚îÄ‚îÄ train_20250912_143022.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ validation_20250912_143022.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ dataset_info_20250912_143022.json
‚îú‚îÄ‚îÄ checkpoints/            # Training checkpoints
‚îú‚îÄ‚îÄ logs/                  # Fine-tuning logs
‚îÇ   ‚îî‚îÄ‚îÄ fine_tuning_20250912_143022.log
‚îî‚îÄ‚îÄ temp/                 # Temporary files
```

## Monitoring & Validation

### Real-Time Monitoring

#### Check Fine-Tuning Status
```bash
# Get current fine-tuning system status
python3 -c "
from pipeline_integration import get_fine_tuning_status
import json
status = get_fine_tuning_status()
print(json.dumps(status, indent=2))
"
```

#### Monitor Active Fine-Tuning
```bash
# Watch fine-tuning logs in real-time
tail -f fine-tuning/logs/fine_tuning_$(date +%Y%m%d)*.log

# Check GPU/MLX utilization
python3 -c "
import mlx.core as mx
print(f'MLX memory usage: {mx.metal.get_peak_memory() / 1e9:.2f}GB')
"
```

### Validation & Testing

#### Test Integration Health
```bash
# Run comprehensive integration tests
bash scripts/tests/test-fine-tuning-phase3.sh

# Expected: 7/7 tests passing
```

#### Validate Fine-Tuned Models
```bash
# List available fine-tuned models
ls -la ~/shared-olmo-models/fine-tuned/

# Test model loading
python3 -c "
from model_manager import OLMoModelManager
manager = OLMoModelManager()
models = manager.list_fine_tuned_models()
print('Available fine-tuned models:')
for model in models:
    print(f'  - {model}')
"
```

## Output & Results

### Generated Artifacts

#### 1. Training Datasets
- **Location**: `fine-tuning/training_data/`
- **Format**: JSONL with prompt-completion pairs
- **Split**: 80% training, 20% validation
- **Metadata**: Dataset info with statistics

#### 2. Fine-Tuned Models
- **Location**: `~/shared-olmo-models/fine-tuned/`
- **Format**: MLX-optimized model files
- **Components**: weights.safetensors, tokenizer, config
- **Naming**: `webauthn-security-olmo-YYYYMMDD-HHMMSS`

#### 3. HuggingFace Uploads
- **Dataset**: `hitoshura25/webauthn-security-vulnerabilities-olmo` (training data, Phase 4)
- **Models**: `hitoshura25/webauthn-security-model-YYYYMMDD-HHMMSS` (fine-tuned models, Phase 3)
- **Upload Triggers**: 
  - `--upload-model` CLI flag (overrides config)
  - `upload_enabled: true` in configuration
  - Requires `HF_TOKEN` environment variable
- **Visibility**: Public (research contribution)

### Performance Metrics

#### Typical Performance (Apple Silicon M1/M2/M3)
- **Fine-Tuning Speed**: 20-30X faster than CPU
- **Memory Usage**: 2-4GB for OLMo-2-1B
- **Fine-Tuning Duration**: 5-15 minutes (100-500 examples)
- **Model Size**: ~1.2GB fine-tuned model

#### Quality Metrics
- **Training Loss**: Monitored during fine-tuning
- **Validation Accuracy**: Computed on validation set
- **Domain Adaptation**: Improved WebAuthn security analysis

## Error Handling & Troubleshooting

### Error Categories

#### 1. Legitimate Opt-Outs (Graceful Handling)
- ‚úÖ **CLI Flag**: `--skip-fine-tuning` (development mode)
- ‚úÖ **Config Override**: `skip_in_daemon: true` (emergency)
- ‚úÖ **MLX Unavailable**: Platform limitation (handled gracefully)
- ‚úÖ **Missing Data**: No training data available

#### 2. Critical Failures (Fail-Fast)
- ‚ùå **System Errors**: Disk space, permissions, memory issues
- ‚ùå **Configuration Corruption**: Invalid YAML, missing required settings  
- ‚ùå **Model Corruption**: Base model files damaged
- ‚ùå **Runtime Errors**: Unexpected MLX failures

### Common Issues & Solutions

#### Issue: "MLX not available" 
```bash
# Solution: Install MLX (see MLX Installation Guide)
pip install mlx mlx-lm
```

#### Issue: Fine-tuning fails with memory error
```bash
# Solution: Reduce batch size in config
# config/olmo-security-config.yaml:
fine_tuning:
  batch_size: 1  # Reduce from default 2
```

#### Issue: Model upload fails
```bash
# Solution: Check HuggingFace token
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
# Or disable upload:
export FINE_TUNING_UPLOAD_ENABLED=false
```

#### Issue: Training dataset empty
```bash
# Solution: Check security analysis phase
# Ensure vulnerabilities were found and analyzed in Phase 1-2
python3 process_artifacts.py --artifacts-dir data/security_artifacts
```

### Debug Commands

#### Comprehensive System Check
```bash
# Run full system diagnostics
python3 -c "
from pipeline_integration import get_fine_tuning_status
from fine_tuning_config import FineTuningConfig
import json

print('=== Fine-Tuning System Diagnostics ===')
status = get_fine_tuning_status()
print(f'Status: {json.dumps(status, indent=2)}')

try:
    config = FineTuningConfig.load_from_config()
    print(f'Config loaded: ‚úÖ')
    print(f'Base model: {config.base_model_name}')
    print(f'Workspace: {config.workspace_dir}')
except Exception as e:
    print(f'Config error: ‚ùå {e}')
"
```

## Best Practices

### Development Workflow
1. **Use opt-out during development**: `--skip-fine-tuning` for faster iteration
2. **Test with small datasets**: Validate pipeline before large runs  
3. **Monitor resource usage**: Check memory/disk space during fine-tuning
4. **Validate models**: Test fine-tuned models before deployment

### Production Deployment
1. **Enable daemon mode**: Automated fine-tuning on production security data
2. **Monitor logs**: Regular log review for fine-tuning health
3. **Resource planning**: Ensure adequate disk space for models
4. **Backup models**: Fine-tuned models are valuable research assets

### Security Considerations
1. **Data Privacy**: Fine-tuned models may contain security data patterns
2. **Model Access**: Restrict access to fine-tuned model directory
3. **Upload Review**: Review model cards before public HuggingFace upload
4. **Local Processing**: All fine-tuning happens locally (no external API calls)

---

**Last Updated**: 2025-09-14  
**Version**: 1.0  
**Related Documents**: 
- `docs/development/mlx-installation-guide.md`
- `docs/improvements/planned/ai-security-fine-tuning-implementation.md`
- `docs/improvements/in-progress/ai-security-fine-tuning-implementation-progress.md`