# AI Security Fine-Tuning Implementation Plan

## Executive Summary

This document provides a comprehensive, test-driven implementation plan to add fine-tuning capabilities to the AI Security Dataset Research Initiative. The plan builds upon the existing 4-phase pipeline (Analysis â†’ Narrativization â†’ Fine-tuning dataset â†’ HuggingFace Upload) by implementing the missing `mlx_finetuning.py` component and establishing complete fine-tuning workflow with MLX optimization.

**Prerequisites**: AI Security Portability Implementation must be completed first (shared model architecture required for fine-tuned model management).

## Current System Analysis

### âœ… Working Components (Post-Portability)
- **Complete 3-phase pipeline**: Analysis â†’ Narrativization â†’ Fine-tuning dataset creation
- **Production dataset**: Live at `hitoshura25/webauthn-security-vulnerabilities-olmo`  
- **MLX optimization**: 214.6 tokens/sec on Apple Silicon (20-30X improvement)
- **Portable configuration**: Shared models at `~/shared-olmo-models/base/` and `~/shared-olmo-models/fine-tuned/`
- **Local automation**: LaunchAgent daemon with portable paths

### ðŸš¨ Missing Component: Fine-Tuning Pipeline
**Gap identified**: Missing `mlx_finetuning.py` component prevents completion of 4th phase
- **Dataset ready**: High-quality security vulnerability narratives available
- **Base models available**: OLMo-2-1B variants optimized for MLX
- **Target architecture**: Fine-tuned models stored in `~/shared-olmo-models/fine-tuned/`
- **Missing implementation**: MLX-optimized fine-tuning process

## Architectural Decision: MLX-Native Fine-Tuning

### Key Insights
1. **MLX Performance Critical**: 20-30X speed improvement must be maintained in fine-tuning
2. **Model Sharing Architecture**: Fine-tuned models become shareable across projects
3. **WebAuthn Domain Specialization**: Fine-tuned model specifically for security vulnerability analysis
4. **Resource Efficiency**: MLX enables on-device fine-tuning with reasonable compute requirements
5. **Open Sharing Mission**: Fine-tuned models contribute to AI security research community

### Chosen Architecture: **MLX-Optimized Fine-Tuning with Shared Model Output**

```
~/shared-olmo-models/
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ OLMo-2-1B-mlx-q4/      # Base model (existing)
â”‚   â””â”€â”€ OLMo-2-1B-mlx/         # Base model (existing)  
â””â”€â”€ fine-tuned/                # Fine-tuned models (NEW)
    â”œâ”€â”€ webauthn-security-v1/   # This project's fine-tuned model
    â”‚   â”œâ”€â”€ model/             # MLX-optimized fine-tuned weights
    â”‚   â”œâ”€â”€ tokenizer/         # Associated tokenizer
    â”‚   â”œâ”€â”€ config/            # Model configuration
    â”‚   â””â”€â”€ metadata.json     # Training metadata, dataset info, performance metrics
    â””â”€â”€ future-models/         # Additional domain-specific models

security-ai-analysis/          # Fine-tuning workspace
â”œâ”€â”€ fine-tuning/               # Fine-tuning specific components (.gitignored)
â”‚   â”œâ”€â”€ training_data/        # Processed training datasets
â”‚   â”œâ”€â”€ checkpoints/          # Training checkpoints during fine-tuning
â”‚   â”œâ”€â”€ logs/                 # Training logs and metrics
â”‚   â””â”€â”€ temp/                 # Temporary training files
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ mlx_finetuning.py     # Main fine-tuning script (NEW)
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ test-fine-tuning.sh # Fine-tuning validation tests
â””â”€â”€ [existing components]     # config_manager.py, etc.
```

**Benefits**:
- **MLX Performance**: Maintains 20-30X speed advantage during training
- **Shared Output**: Fine-tuned models available across projects
- **Incremental Training**: Can create specialized variants (webauthn-security-v2, etc.)
- **Community Contribution**: Fine-tuned models publishable to HuggingFace
- **Resource Efficient**: On-device training with Apple Silicon optimization

## Test-Driven Implementation Plan

### Phase 1: Fine-Tuning Infrastructure & Configuration (4-6 hours)

**Objective**: Establish MLX fine-tuning infrastructure with portable configuration

#### 1.1 Enhanced Configuration for Fine-Tuning
**File**: `config/olmo-security-config.yaml` (extend existing)
```yaml
# Existing configuration...
base_models_dir: "~/shared-olmo-models/base"
fine_tuned_models_dir: "~/shared-olmo-models/fine-tuned"
default_base_model: "OLMo-2-1B-mlx-q4"

# Fine-tuning configuration (NEW)
fine_tuning:
  workspace_dir: "security-ai-analysis/fine-tuning"
  default_output_name: "webauthn-security-v1"
  
  # Training hyperparameters
  training:
    learning_rate: 2e-5
    batch_size: 4
    max_epochs: 3
    warmup_steps: 100
    save_steps: 500
    eval_steps: 250
    
  # MLX-specific settings
  mlx:
    quantization: "q4"  # Match base model quantization
    memory_efficient: true
    gradient_checkpointing: true
```

#### 1.2 Fine-Tuning Configuration Manager
**File**: `security-ai-analysis/fine_tuning_config.py`
```python
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, Any, Optional
import yaml

@dataclass
class FineTuningConfig:
    """Configuration for MLX fine-tuning process"""
    
    # Directories
    workspace_dir: Path
    base_models_dir: Path
    fine_tuned_models_dir: Path
    
    # Model settings
    base_model_name: str
    output_model_name: str
    
    # Training parameters
    learning_rate: float
    batch_size: int
    max_epochs: int
    warmup_steps: int
    save_steps: int
    eval_steps: int
    
    # MLX settings
    quantization: str
    memory_efficient: bool
    gradient_checkpointing: bool
    
    @classmethod
    def load_from_config(cls, config_file: Optional[Path] = None) -> 'FineTuningConfig':
        """Load fine-tuning configuration from YAML file"""
        if config_file is None:
            config_file = Path(__file__).parent.parent / "config" / "olmo-security-config.yaml"
        
        with open(config_file, 'r') as f:
            config = yaml.safe_load(f)
        
        ft_config = config['fine_tuning']
        
        # Resolve paths
        project_root = Path(__file__).parent.parent
        workspace_dir = project_root / ft_config['workspace_dir']
        base_models_dir = Path(config['base_models_dir']).expanduser()
        fine_tuned_models_dir = Path(config['fine_tuned_models_dir']).expanduser()
        
        return cls(
            workspace_dir=workspace_dir,
            base_models_dir=base_models_dir,
            fine_tuned_models_dir=fine_tuned_models_dir,
            base_model_name=config['default_base_model'],
            output_model_name=ft_config['default_output_name'],
            learning_rate=ft_config['training']['learning_rate'],
            batch_size=ft_config['training']['batch_size'],
            max_epochs=ft_config['training']['max_epochs'],
            warmup_steps=ft_config['training']['warmup_steps'],
            save_steps=ft_config['training']['save_steps'],
            eval_steps=ft_config['training']['eval_steps'],
            quantization=ft_config['mlx']['quantization'],
            memory_efficient=ft_config['mlx']['memory_efficient'],
            gradient_checkpointing=ft_config['mlx']['gradient_checkpointing']
        )
    
    def get_base_model_path(self) -> Path:
        """Get path to base model for fine-tuning"""
        model_path = self.base_models_dir / self.base_model_name
        if not model_path.exists():
            raise FileNotFoundError(f"Base model not found: {model_path}")
        return model_path
    
    def get_output_model_path(self) -> Path:
        """Get path where fine-tuned model will be saved"""
        return self.fine_tuned_models_dir / self.output_model_name
    
    def setup_workspace(self):
        """Create fine-tuning workspace directories"""
        for subdir in ['training_data', 'checkpoints', 'logs', 'temp']:
            (self.workspace_dir / subdir).mkdir(parents=True, exist_ok=True)
```

#### 1.3 Phase 1 Validation Tests
**Test Script**: `security-ai-analysis/scripts/tests/test-fine-tuning-phase1.sh`
```bash
#!/bin/bash
set -euo pipefail

echo "ðŸ§ª Fine-Tuning Phase 1 Validation Tests"

# Test 1: Fine-tuning config loads without errors
python3 -c "
from security_ai_analysis.fine_tuning_config import FineTuningConfig
config = FineTuningConfig.load_from_config()
print(f'Base model: {config.base_model_name}')
print(f'Output model: {config.output_model_name}')
print(f'Workspace: {config.workspace_dir}')
print('âœ… Fine-tuning config loads successfully')
"

# Test 2: Workspace directories created
python3 -c "
from security_ai_analysis.fine_tuning_config import FineTuningConfig
config = FineTuningConfig.load_from_config()
config.setup_workspace()
required_dirs = ['training_data', 'checkpoints', 'logs', 'temp']
for dir_name in required_dirs:
    dir_path = config.workspace_dir / dir_name
    if not dir_path.exists():
        raise AssertionError(f'Required directory not created: {dir_path}')
print('âœ… Workspace directories created successfully')
"

# Test 3: Base model path resolution works
python3 -c "
from security_ai_analysis.fine_tuning_config import FineTuningConfig
config = FineTuningConfig.load_from_config()
try:
    base_path = config.get_base_model_path()
    print(f'âœ… Base model found at: {base_path}')
except FileNotFoundError as e:
    print(f'âš ï¸  Base model not found (expected if not downloaded): {e}')
"

echo "âœ… Fine-Tuning Phase 1 validation complete"
```

### Phase 2: MLX Fine-Tuning Engine Implementation (6-8 hours)

**Objective**: Implement core MLX-optimized fine-tuning functionality

#### 2.1 MLX Fine-Tuning Core Engine
**File**: `security-ai-analysis/scripts/mlx_finetuning.py`
```python
#!/usr/bin/env python3
"""
MLX-Optimized Fine-Tuning for OLMo Security Models

Implements fine-tuning of OLMo-2-1B models using MLX framework for Apple Silicon optimization.
Designed for security vulnerability analysis domain specialization.
"""

import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import asdict
import time
from datetime import datetime

# MLX imports (requires MLX installation)
try:
    import mlx.core as mx
    import mlx.nn as nn
    import mlx.optimizers as optim
    from mlx_lm import load, generate, convert, fine_tune
    MLX_AVAILABLE = True
except ImportError:
    MLX_AVAILABLE = False
    print("âš ï¸  MLX not available. Fine-tuning requires MLX installation on Apple Silicon.")

# Add parent directory for imports
sys.path.append(str(Path(__file__).parent.parent))
from fine_tuning_config import FineTuningConfig

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MLXFineTuner:
    """MLX-optimized fine-tuning for OLMo security models"""
    
    def __init__(self, config: Optional[FineTuningConfig] = None):
        if not MLX_AVAILABLE:
            raise RuntimeError("MLX not available. Fine-tuning requires MLX on Apple Silicon.")
            
        self.config = config or FineTuningConfig.load_from_config()
        self.config.setup_workspace()
        
        # Setup logging for this session
        log_file = self.config.workspace_dir / "logs" / f"fine_tuning_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        logger.addHandler(file_handler)
        
        logger.info(f"MLX Fine-Tuner initialized with config: {self.config.output_model_name}")
        
    def prepare_training_data(self, dataset_file: Path) -> Path:
        """
        Prepare training data from HuggingFace dataset format to MLX format
        
        Expected input: JSON lines with 'instruction' and 'response' fields
        Output: MLX-compatible training format
        """
        logger.info(f"Preparing training data from: {dataset_file}")
        
        if not dataset_file.exists():
            raise FileNotFoundError(f"Dataset file not found: {dataset_file}")
        
        # Output path
        output_file = self.config.workspace_dir / "training_data" / "mlx_training_data.jsonl"
        
        with open(dataset_file, 'r') as infile, open(output_file, 'w') as outfile:
            for line_num, line in enumerate(infile, 1):
                try:
                    data = json.loads(line.strip())
                    
                    # Convert to MLX fine-tuning format
                    mlx_format = {
                        "messages": [
                            {"role": "user", "content": data["instruction"]},
                            {"role": "assistant", "content": data["response"]}
                        ]
                    }
                    
                    outfile.write(json.dumps(mlx_format) + "\\n")
                    
                except json.JSONDecodeError as e:
                    logger.warning(f"Skipping invalid JSON at line {line_num}: {e}")
                    continue
        
        logger.info(f"Training data prepared: {output_file}")
        return output_file
    
    def run_fine_tuning(self, training_data_file: Path) -> Path:
        """
        Execute MLX fine-tuning process
        
        Returns path to fine-tuned model
        """
        logger.info("Starting MLX fine-tuning process")
        
        base_model_path = self.config.get_base_model_path()
        output_model_path = self.config.get_output_model_path()
        
        # Ensure output directory exists
        output_model_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Base model: {base_model_path}")
        logger.info(f"Output model: {output_model_path}")
        logger.info(f"Training data: {training_data_file}")
        
        # MLX fine-tuning parameters
        fine_tune_args = {
            "model": str(base_model_path),
            "data": str(training_data_file),
            "output_dir": str(output_model_path),
            "learning_rate": self.config.learning_rate,
            "batch_size": self.config.batch_size,
            "max_epochs": self.config.max_epochs,
            "warmup_steps": self.config.warmup_steps,
            "save_steps": self.config.save_steps,
            "eval_steps": self.config.eval_steps,
            "gradient_checkpointing": self.config.gradient_checkpointing,
            "quantization": self.config.quantization if self.config.quantization != "none" else None
        }
        
        logger.info(f"Fine-tuning parameters: {fine_tune_args}")
        
        try:
            # Start fine-tuning timer
            start_time = time.time()
            
            # Execute MLX fine-tuning
            # Note: This is a placeholder for actual MLX fine-tuning API
            # Actual implementation will depend on MLX-LM fine-tuning interface
            logger.info("Executing MLX fine-tuning...")
            
            # Placeholder for actual fine-tuning call:
            # fine_tune.train(**fine_tune_args)
            
            # For now, create placeholder structure
            self._create_model_structure(output_model_path, fine_tune_args)
            
            end_time = time.time()
            training_duration = end_time - start_time
            
            logger.info(f"Fine-tuning completed in {training_duration:.2f} seconds")
            
            # Save training metadata
            self._save_training_metadata(output_model_path, fine_tune_args, training_duration)
            
            return output_model_path
            
        except Exception as e:
            logger.error(f"Fine-tuning failed: {e}")
            raise
    
    def _create_model_structure(self, output_path: Path, training_args: Dict[str, Any]):
        """Create fine-tuned model directory structure"""
        
        # Create subdirectories
        (output_path / "model").mkdir(exist_ok=True)
        (output_path / "tokenizer").mkdir(exist_ok=True)
        (output_path / "config").mkdir(exist_ok=True)
        
        # Placeholder files (actual MLX fine-tuning would create these)
        (output_path / "model" / "weights.safetensors").touch()
        (output_path / "tokenizer" / "tokenizer.json").touch()
        (output_path / "config" / "config.json").touch()
        
        logger.info(f"Model structure created at: {output_path}")
    
    def _save_training_metadata(self, output_path: Path, training_args: Dict[str, Any], duration: float):
        """Save training metadata for future reference"""
        
        metadata = {
            "model_name": self.config.output_model_name,
            "base_model": self.config.base_model_name,
            "training_date": datetime.now().isoformat(),
            "training_duration_seconds": duration,
            "training_parameters": training_args,
            "config": asdict(self.config),
            "mlx_version": "placeholder",  # Would get actual MLX version
            "performance_notes": "MLX-optimized for Apple Silicon"
        }
        
        metadata_file = output_path / "metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"Training metadata saved: {metadata_file}")
    
    def validate_fine_tuned_model(self, model_path: Path) -> bool:
        """
        Validate fine-tuned model structure and basic functionality
        
        Returns True if model appears valid
        """
        logger.info(f"Validating fine-tuned model: {model_path}")
        
        required_files = [
            "model/weights.safetensors",
            "tokenizer/tokenizer.json", 
            "config/config.json",
            "metadata.json"
        ]
        
        missing_files = []
        for file_path in required_files:
            full_path = model_path / file_path
            if not full_path.exists():
                missing_files.append(str(full_path))
        
        if missing_files:
            logger.error(f"Missing required files: {missing_files}")
            return False
        
        # Validate metadata
        try:
            metadata_file = model_path / "metadata.json"
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
            
            required_metadata = ["model_name", "base_model", "training_date", "training_parameters"]
            missing_metadata = [key for key in required_metadata if key not in metadata]
            
            if missing_metadata:
                logger.error(f"Missing required metadata: {missing_metadata}")
                return False
                
        except (json.JSONDecodeError, FileNotFoundError) as e:
            logger.error(f"Invalid metadata file: {e}")
            return False
        
        logger.info("âœ… Fine-tuned model validation passed")
        return True

def main():
    """Main fine-tuning entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="MLX-Optimized Fine-Tuning for OLMo Security Models",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=\"\"\"
Examples:
  # Fine-tune with default dataset
  python3 mlx_finetuning.py --dataset data/security_vulnerabilities_dataset.jsonl
  
  # Fine-tune with custom model name
  python3 mlx_finetuning.py --dataset data/custom_dataset.jsonl --output-name webauthn-security-v2
  
  # Validate existing fine-tuned model
  python3 mlx_finetuning.py --validate-model ~/shared-olmo-models/fine-tuned/webauthn-security-v1
        \"\"\"
    )
    
    parser.add_argument("--dataset", type=Path, 
                       help="Path to training dataset (JSONL format)")
    parser.add_argument("--output-name", 
                       help="Name for fine-tuned model (default from config)")
    parser.add_argument("--config", type=Path,
                       help="Path to configuration file")
    parser.add_argument("--validate-model", type=Path,
                       help="Validate existing fine-tuned model")
    
    args = parser.parse_args()
    
    try:
        # Load configuration
        config = None
        if args.config:
            config = FineTuningConfig.load_from_config(args.config)
        else:
            config = FineTuningConfig.load_from_config()
            
        if args.output_name:
            config.output_model_name = args.output_name
        
        fine_tuner = MLXFineTuner(config)
        
        if args.validate_model:
            # Validate existing model
            is_valid = fine_tuner.validate_fine_tuned_model(args.validate_model)
            return 0 if is_valid else 1
            
        elif args.dataset:
            # Run fine-tuning
            logger.info(f"Starting fine-tuning with dataset: {args.dataset}")
            
            # Prepare training data
            training_data = fine_tuner.prepare_training_data(args.dataset)
            
            # Execute fine-tuning
            output_model = fine_tuner.run_fine_tuning(training_data)
            
            # Validate result
            if fine_tuner.validate_fine_tuned_model(output_model):
                logger.info(f"âœ… Fine-tuning completed successfully: {output_model}")
                return 0
            else:
                logger.error("âŒ Fine-tuning validation failed")
                return 1
        else:
            parser.print_help()
            return 1
            
    except Exception as e:
        logger.error(f"Fine-tuning failed: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
```

#### 2.2 Phase 2 Validation Tests
**Test Script**: `security-ai-analysis/scripts/tests/test-fine-tuning-phase2.sh`
```bash
#!/bin/bash
set -euo pipefail

echo "ðŸ§ª Fine-Tuning Phase 2 Validation Tests"

# Activate virtual environment
source security-ai-analysis/venv/bin/activate
cd security-ai-analysis

# Test 1: MLX fine-tuning script loads without errors
python3 -c "
import sys
sys.path.append('scripts')
from mlx_finetuning import MLXFineTuner, FineTuningConfig
config = FineTuningConfig.load_from_config()
print(f'âœ… MLX fine-tuning imports successful')
print(f'Base model: {config.base_model_name}')
print(f'Output model: {config.output_model_name}')
"

# Test 2: Training data preparation (mock data)
echo "Creating mock training dataset..."
mkdir -p fine-tuning/training_data
cat > fine-tuning/training_data/mock_dataset.jsonl << 'EOF'
{"instruction": "Analyze this security vulnerability", "response": "This is a mock security analysis response"}
{"instruction": "Explain this WebAuthn issue", "response": "This is a mock WebAuthn explanation"}
EOF

python3 -c "
import sys
sys.path.append('scripts')
from mlx_finetuning import MLXFineTuner
from pathlib import Path

fine_tuner = MLXFineTuner()
dataset_file = Path('fine-tuning/training_data/mock_dataset.jsonl')
prepared_data = fine_tuner.prepare_training_data(dataset_file)
print(f'âœ… Training data preparation successful: {prepared_data}')
"

# Test 3: Model structure creation (without actual training)
python3 -c "
import sys
sys.path.append('scripts')
from mlx_finetuning import MLXFineTuner
from pathlib import Path

fine_tuner = MLXFineTuner()
test_output_path = Path('fine-tuning/temp/test_model')
test_output_path.mkdir(parents=True, exist_ok=True)

# Test model structure creation
training_args = {'test': 'placeholder'}
fine_tuner._create_model_structure(test_output_path, training_args)
fine_tuner._save_training_metadata(test_output_path, training_args, 123.45)

# Validate structure
is_valid = fine_tuner.validate_fine_tuned_model(test_output_path)
if not is_valid:
    raise AssertionError('Model structure validation failed')
print('âœ… Model structure creation and validation successful')
"

# Test 4: Command-line interface
python3 scripts/mlx_finetuning.py --help | grep -q "MLX-Optimized Fine-Tuning" && echo "âœ… CLI interface works"

echo "âœ… Fine-Tuning Phase 2 validation complete"
```

### Phase 3: Dataset Integration & Pipeline Enhancement (3-4 hours)

**Objective**: Integrate fine-tuning into existing 4-phase pipeline

#### 3.1 Enhanced Process Artifacts Script
**Updates to**: `security-ai-analysis/process_artifacts.py` (extend existing)

Add fine-tuning integration to complete the 4-phase pipeline:
```python
# Add to existing process_artifacts.py

def run_fine_tuning_phase(artifacts_dir: Path, output_dir: Path, model_name: str) -> bool:
    """
    Phase 4: Fine-tuning (NEW)
    Run MLX fine-tuning on prepared dataset
    """
    logger.info("ðŸ¤– Phase 4: Fine-tuning OLMo model...")
    
    # Find prepared dataset from Phase 3
    dataset_file = output_dir / "huggingface_dataset" / "train.jsonl"
    if not dataset_file.exists():
        logger.error(f"Training dataset not found: {dataset_file}")
        return False
    
    # Fine-tuning output directory
    fine_tuning_dir = output_dir / "fine_tuning"
    fine_tuning_dir.mkdir(exist_ok=True)
    
    try:
        # Import and run fine-tuning
        from scripts.mlx_finetuning import MLXFineTuner, FineTuningConfig
        
        # Load config and customize output name
        config = FineTuningConfig.load_from_config()
        
        # Generate unique model name with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        config.output_model_name = f"webauthn-security-{timestamp}"
        
        # Execute fine-tuning
        fine_tuner = MLXFineTuner(config)
        fine_tuned_model = fine_tuner.run_fine_tuning(dataset_file)
        
        logger.info(f"âœ… Fine-tuning completed: {fine_tuned_model}")
        
        # Log model information
        metadata_file = fine_tuned_model / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
            logger.info(f"Fine-tuned model: {metadata['model_name']}")
            logger.info(f"Training duration: {metadata['training_duration_seconds']:.2f}s")
        
        return True
        
    except Exception as e:
        logger.error(f"Fine-tuning failed: {e}")
        return False

# Update main pipeline to include fine-tuning
def main():
    # ... existing code ...
    
    # Add fine-tuning option
    parser.add_argument("--enable-fine-tuning", action="store_true",
                       help="Enable fine-tuning phase (requires prepared dataset)")
    
    # ... existing pipeline phases 1-3 ...
    
    # Phase 4: Fine-tuning (NEW)
    if args.enable_fine_tuning:
        fine_tuning_success = run_fine_tuning_phase(artifacts_dir, output_dir, args.model_name)
        if not fine_tuning_success:
            logger.error("Fine-tuning phase failed")
            return 1
        
        pipeline_phases.append("Fine-tuning")
    
    # Final summary
    logger.info(f"ðŸŽ‰ Complete pipeline finished: {' â†’ '.join(pipeline_phases)}")
    return 0
```

#### 3.2 LaunchAgent Integration
**Updates to**: `local-analysis/security_artifact_daemon.py`

Add fine-tuning capability to daemon workflow:
```python
# Add to security_artifact_daemon.py

def trigger_local_analysis(self, artifacts_dir: Path, run_info: Dict) -> bool:
    """Enhanced to include optional fine-tuning"""
    logger.info(f"ðŸ¤– Starting local OLMo analysis for run {run_info['run_id']}...")
    
    # ... existing analysis code ...
    
    # Enhanced analysis command with fine-tuning option
    analysis_command = [
        sys.executable, str(process_script),
        "--local-mode",
        "--artifacts-dir", str(artifacts_dir),
        "--output-dir", str(analysis_output),
        "--model-name", str(config.get_base_model_path()),
        "--branch", "main", 
        "--commit", run_info['head_sha'],
        "--enable-fine-tuning"  # Enable fine-tuning in automated workflow
    ]
    
    # ... rest of existing code ...
```

#### 3.3 Phase 3 Validation Tests  
**Test Script**: `security-ai-analysis/scripts/tests/test-fine-tuning-phase3.sh`
```bash
#!/bin/bash
set -euo pipefail

echo "ðŸ§ª Fine-Tuning Phase 3 Validation Tests"

# Test 1: Enhanced process_artifacts includes fine-tuning
source security-ai-analysis/venv/bin/activate
cd security-ai-analysis

python3 process_artifacts.py --help | grep -q "enable-fine-tuning" && echo "âœ… Fine-tuning option added to process_artifacts"

# Test 2: Mock end-to-end pipeline with fine-tuning
echo "Creating mock pipeline test..."
mkdir -p test_pipeline_data
echo '{"test": "vulnerability"}' > test_pipeline_data/mock_artifact.json

python3 process_artifacts.py \\
  --local-mode \\
  --artifacts-dir test_pipeline_data \\
  --output-dir test_pipeline_output \\
  --model-name OLMo-2-1B-mlx-q4 \\
  --enable-fine-tuning \\
  --dry-run && echo "âœ… Enhanced pipeline command structure works"

# Test 3: Daemon integration check
cd ../local-analysis
python3 -c "
import sys
sys.path.append('../security-ai-analysis/scripts')
from security_artifact_daemon import SecurityArtifactDaemon
daemon = SecurityArtifactDaemon()
print('âœ… Daemon fine-tuning integration compatible')
"

echo "âœ… Fine-Tuning Phase 3 validation complete"
```

### Phase 4: Documentation & MLX Dependencies (2-3 hours)

**Objective**: Complete documentation and dependency management

#### 4.1 Enhanced Requirements  
**Updates to**: `security-ai-analysis/requirements.txt`
```txt
# Existing requirements...

# MLX fine-tuning dependencies (Apple Silicon only)
mlx>=0.0.8                    # MLX framework for Apple Silicon
mlx-lm>=0.0.8                 # MLX Language Model utilities
```

#### 4.2 Fine-Tuning Documentation
**File**: `security-ai-analysis/docs/FINE_TUNING.md`
```markdown
# MLX Fine-Tuning Guide

## Overview

This guide covers fine-tuning OLMo models for security vulnerability analysis using MLX optimization on Apple Silicon.

## Prerequisites

1. **AI Security Portability** implementation completed
2. **Apple Silicon Mac** (M1/M2/M3 series)  
3. **MLX framework** installed
4. **Prepared dataset** from security analysis pipeline

## Quick Start

### 1. Setup (if not already done)
```bash
python3 security-ai-analysis/scripts/setup.py
```

### 2. Run Fine-Tuning
```bash
cd security-ai-analysis
source venv/bin/activate

# Fine-tune with existing dataset
python3 scripts/mlx_finetuning.py --dataset data/security_dataset.jsonl

# Or run complete pipeline with fine-tuning
python3 process_artifacts.py --local-mode --artifacts-dir data/ --enable-fine-tuning
```

### 3. Use Fine-Tuned Model
Fine-tuned models are saved to `~/shared-olmo-models/fine-tuned/` and can be used across projects.

## Configuration

Fine-tuning parameters are configured in `config/olmo-security-config.yaml`:

```yaml
fine_tuning:
  training:
    learning_rate: 2e-5
    batch_size: 4  
    max_epochs: 3
```

## Performance

Expected fine-tuning performance on Apple Silicon:
- **M1/M2**: ~2-4 hours for typical security dataset
- **MLX Optimization**: 20-30X faster than CPU training
- **Memory Usage**: ~8-16GB during training

## Troubleshooting

### MLX Not Available
```bash
pip install mlx mlx-lm
```

### Model Not Found
Ensure base model is downloaded:
```bash
python3 scripts/setup.py
```
```

#### 4.3 Updated .gitignore
**Updates to**: `security-ai-analysis/.gitignore`
```gitignore
# Existing entries...

# Fine-tuning workspace (temporary training files)
fine-tuning/training_data/
fine-tuning/checkpoints/
fine-tuning/logs/
fine-tuning/temp/
```

### Phase 5: End-to-End Integration & Performance Testing (4-5 hours)

**Objective**: Comprehensive testing with performance validation

#### 5.1 Complete Integration Test
**Test Script**: `security-ai-analysis/scripts/tests/test-fine-tuning-integration.sh`
```bash
#!/bin/bash
set -euo pipefail

echo "ðŸ§ª Fine-Tuning Complete Integration Test"

# Test in fresh environment
TEST_DIR="/tmp/fine-tuning-integration-test-$(date +%s)"
mkdir -p "$TEST_DIR" 
cd "$TEST_DIR"

# Clone and setup
git clone https://github.com/hitoshura25/mpo-api-authn-server.git
cd mpo-api-authn-server

# Setup system
python3 security-ai-analysis/scripts/setup.py

# Create realistic test dataset
source security-ai-analysis/venv/bin/activate
cd security-ai-analysis

mkdir -p test_data
cat > test_data/security_dataset.jsonl << 'EOF'
{"instruction": "Analyze this SQL injection vulnerability in authentication", "response": "This SQL injection vulnerability occurs in the user authentication process..."}
{"instruction": "Explain the impact of this XSS vulnerability", "response": "This XSS vulnerability allows attackers to inject malicious scripts..."}
{"instruction": "Describe mitigation for this CSRF attack", "response": "To mitigate this CSRF attack, implement proper token validation..."}
EOF

# Test complete pipeline with fine-tuning
echo "Testing complete 4-phase pipeline..."
python3 process_artifacts.py \\
  --local-mode \\
  --artifacts-dir test_data \\
  --output-dir integration_output \\
  --enable-fine-tuning

# Validate outputs
if [[ -d "integration_output/fine_tuning" ]]; then
  echo "âœ… Fine-tuning phase executed"
else
  echo "âŒ Fine-tuning phase missing"
  exit 1
fi

# Check shared model directory
if [[ -d ~/shared-olmo-models/fine-tuned/ ]]; then
  echo "âœ… Fine-tuned models directory exists"
  
  # Look for recent fine-tuned model
  RECENT_MODEL=$(find ~/shared-olmo-models/fine-tuned/ -name "webauthn-security-*" -type d | head -1)
  if [[ -n "$RECENT_MODEL" ]]; then
    echo "âœ… Fine-tuned model found: $(basename "$RECENT_MODEL")"
    
    # Validate model structure
    python3 scripts/mlx_finetuning.py --validate-model "$RECENT_MODEL"
  fi
fi

echo "âœ… Fine-Tuning complete integration test passed"

# Cleanup
cd /
rm -rf "$TEST_DIR"
```

#### 5.2 Performance Benchmarking
**Test Script**: `security-ai-analysis/scripts/tests/test-fine-tuning-performance.sh`
```bash
#!/bin/bash
set -euo pipefail

echo "ðŸ§ª Fine-Tuning Performance Benchmarking"

source security-ai-analysis/venv/bin/activate
cd security-ai-analysis

# Create performance test dataset
mkdir -p performance_test_data
echo "Generating larger test dataset for performance testing..."

# Generate 100 training examples
for i in {1..100}; do
  echo '{"instruction": "Analyze security vulnerability #'$i'", "response": "This is a detailed security analysis response for vulnerability '$i' that covers the technical details, impact assessment, and recommended remediation steps..."}' >> performance_test_data/perf_dataset.jsonl
done

echo "Created performance dataset with 100 examples"

# Run performance test
echo "Starting performance benchmark..."
START_TIME=$(date +%s)

python3 scripts/mlx_finetuning.py \\
  --dataset performance_test_data/perf_dataset.jsonl \\
  --output-name webauthn-security-perf-test

END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))

echo "Fine-tuning completed in $DURATION seconds"

# Performance validation
if [[ $DURATION -lt 7200 ]]; then  # Less than 2 hours
  echo "âœ… Performance target met: $DURATION seconds (< 2 hours)"
else
  echo "âš ï¸  Performance slower than expected: $DURATION seconds"
fi

# Validate model quality (basic checks)
PERF_MODEL=~/shared-olmo-models/fine-tuned/webauthn-security-perf-test
if python3 scripts/mlx_finetuning.py --validate-model "$PERF_MODEL"; then
  echo "âœ… Performance test model validation passed"
else
  echo "âŒ Performance test model validation failed"
  exit 1
fi

echo "âœ… Fine-Tuning performance benchmarking complete"
```

## Risk Mitigation & Success Criteria

### Low-Risk Approach
- **Prerequisites enforced**: Portability implementation required first
- **Incremental phases**: Each phase independently testable
- **MLX compatibility**: Graceful fallback if MLX unavailable  
- **Configuration-driven**: All parameters tunable without code changes

### Success Criteria

#### Must Pass All Tests
1. **Phase 1**: Fine-tuning configuration loads, workspace creation works
2. **Phase 2**: MLX fine-tuning engine functional, model structure creation works
3. **Phase 3**: Pipeline integration successful, daemon compatibility maintained
4. **Phase 4**: Documentation complete, dependencies resolved
5. **Phase 5**: End-to-end integration works, performance targets met

#### Performance Requirements
- **MLX Performance**: Fine-tuning completes within 2 hours for typical dataset
- **Model Quality**: Fine-tuned model passes validation checks
- **Pipeline Integration**: All 4 phases work together seamlessly
- **Shared Model Architecture**: Fine-tuned models available across projects

#### Technical Requirements
- **Apple Silicon Compatibility**: Works on M1/M2/M3 Macs with MLX
- **Memory Efficiency**: Fine-tuning uses <16GB RAM
- **Model Sharing**: Fine-tuned models stored in shared directory structure
- **Configuration Driven**: All parameters configurable via YAML

## Implementation Timeline

**Total Estimated Time**: 19-26 hours across multiple sessions
- **Phase 1**: 4-6 hours (infrastructure & configuration)
- **Phase 2**: 6-8 hours (MLX fine-tuning engine)  
- **Phase 3**: 3-4 hours (pipeline integration)
- **Phase 4**: 2-3 hours (documentation & dependencies)
- **Phase 5**: 4-5 hours (integration & performance testing)

## For Future Claude Sessions

### Context Summary
- **Prerequisite**: AI Security Portability implementation provides shared model architecture
- **Architecture**: MLX-optimized fine-tuning with shared model output
- **4-phase completion**: Analysis â†’ Narrativization â†’ Fine-tuning dataset â†’ **Fine-tuning** (NEW)
- **Test-driven**: Each phase has validation script that must pass

### Current Status Tracking
- **Branch**: Feature branch with fine-tuning implementation plan
- **Dependencies**: Requires completed portability implementation
- **Next**: Begin Phase 1 implementation following detailed specifications

**âš ï¸ Critical**: Fine-tuning requires Apple Silicon with MLX framework. Ensure hardware compatibility before implementation.

---

**ðŸ“… Created**: 2025-09-10  
**ðŸ“‹ Prerequisites**: AI Security Portability Implementation (must be completed first)  
**ðŸŽ¯ Objective**: Complete 4-phase pipeline with MLX-optimized fine-tuning capability