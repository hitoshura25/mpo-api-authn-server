# AI Security Analysis Configuration - Minimal Required Values Only
#
# All other values use code defaults:
# - HuggingFace upload: enabled by default (can be disabled via CLI)
# - Training parameters: optimized defaults for security analysis
# - MLX settings: optimized for Apple Silicon

# External model directories (shareable across projects)
base_models_dir: "~/shared-olmo-models/base"
fine_tuned_models_dir: "~/shared-olmo-models/fine-tuned"

# Default model optimized for Apple Silicon with MLX
default_base_model: "OLMo-2-1B-Instruct-mlx-q4"

# Fine-tuning configuration
fine_tuning:
  workspace_dir: "security-ai-analysis/fine-tuning"
  default_output_name: "webauthn-security-v1"

  # Training parameters - research-backed optimization for model quality (2024-2025)
  # Sources: Databricks LoRA Guide, Sebastian Raschka's Practical Tips, MLPerf LoRA Study
  training:
    learning_rate: 5e-6    # 2e-5→5e-6: Conservative for CF prevention (Raschka 2024)
    stage2_learning_rate: 1e-6  # Stage 2: 0.2x Stage 1 LR for fine specialization
    batch_size: 1          # KEEP: Optimal for Metal GPU memory efficiency
    max_epochs: 5          # 3→5: More passes over limited dataset
    warmup_steps: 150      # 100→150: Longer warmup for stability
    save_steps: 200        # 500→200: Frequent checkpoints for early CF detection
    eval_steps: 100        # 250→100: Continuous validation monitoring
    max_stage1_iters: 800  # 100→800: 8x increase for deep specialization (Databricks)
    max_stage2_iters: 1200 # 150→1200: 8x increase for complex code generation
    stage1_replay_ratio: 0.15  # Stage 2: 15% Stage 1 data replay for catastrophic forgetting prevention

  # Advanced LoRA configuration - 2024-2025 research optimizations
  # Sources: LoRA original paper, Databricks LoRA Guide, Sebastian Raschka's Practical Tips
  lora:
    rank: 16                   # 8→16: Higher rank for better information capture
    alpha: 32                  # 16→32: Balanced alpha for optimal adaptation
    dropout: 0.1               # 0.05→0.1: Higher dropout for regularization
    # Comprehensive target modules for transformer architecture optimization
    target_modules: [ "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj" ]

  # MLX optimization settings - optimized for Apple Silicon
  mlx:
    quantization: "q4"
    memory_efficient: true
    gradient_checkpointing: true

  # HuggingFace upload settings
  huggingface:
    upload_enabled: true
    default_repo_prefix: "hitoshura25"
    private_repos: false
    skip_in_daemon: false

# Data augmentation configuration - research-backed techniques for small datasets
# Sources: IEEE 2024 (self-mixup), MIT TACL, Theodo Research
augmentation:
  enabled: true                 # Default enabled for quality improvements
  multiplier: 4                 # Target 4x total examples (200 → 800)
  mixup_alpha: 0.3             # Self-mixup blending ratio (IEEE 2024 optimal)
  semantic_variations: 3        # Number of semantic variation types

# Knowledge base configuration
knowledge_base:
  base_dir: "security-ai-analysis/knowledge_base"
  embeddings_model: "sentence-transformers/all-MiniLM-L6-v2"
  vector_store_type: "faiss"

# Model validation configuration - realistic thresholds based on 2024-2025 research
# Sources: Yurts.ai CF Research, ACL 2024 Findings
validation:
  # Production thresholds (stretch goals with enhanced training)
  stage1_threshold: 0.7         # Target production threshold for Stage 1 specialization
  stage2_threshold: 0.7         # Target production threshold for Stage 2 specialization
  sequential_threshold: 0.6     # Target production threshold for CF prevention

# Multi-domain security specialization configuration (always enabled)
multi_domain:
  # Target security categories for specialization
  target_categories:
    - "webauthn_security"
    - "web_security"
    - "code_vulnerabilities"
    - "container_security"
    - "dependency_vulnerabilities"
    - "mobile_security"
    - "infrastructure_security"

  # Category weights for validation scoring (higher weight = more important)
  category_weights:
    webauthn_security: 1.5      # Primary specialization area
    web_security: 1.2           # Related to WebAuthn
    code_vulnerabilities: 1.0   # Standard baseline
    container_security: 0.9
    dependency_vulnerabilities: 0.8
    mobile_security: 0.7
    infrastructure_security: 0.6

  # Validation thresholds for multi-domain specialization
  validation:
    overall_threshold: 0.75     # Target overall multi-domain score (high specialization)
    category_minimum: 0.40      # Minimum score required for any category
    high_specialization: 0.75   # Score threshold for "high specialization"
    medium_specialization: 0.60 # Score threshold for "medium specialization"
